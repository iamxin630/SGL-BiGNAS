import logging

import torch
import torch.nn as nn
import wandb
from sklearn.metrics import roc_auc_score
from torch.utils.data import DataLoader
import numpy as np  
from auxilearn.optim import MetaOptimizer
from dataset import Dataset
from pytorchtools import EarlyStopping
from utils import link_split, load_model
# 1. åœ¨æª”æ¡ˆé–‹é ­çš„ import éƒ¨åˆ†ï¼Œç¢ºä¿é€™è¡Œå­˜åœ¨ï¼š
from sgl2bignas_adapter import load_sgl_final_embeddings, init_bignas_source_from_sgl, step_unfreeze_if_ready
import torch.nn.functional as F
import collections  # <== æ–°å¢ï¼šä¹‹å¾Œç”¨ collections.Counter é¿å…è¢«é™°å½±


def meta_optimizeation(
    target_meta_loader,
    replace_optimizer,
    model,
    args,
    criterion,
    replace_scheduler,
    source_edge_index,
    target_edge_index,
):
    device = args.device
    for batch, (target_link, target_label) in enumerate(target_meta_loader):
        if batch < args.descent_step:
            target_link, target_label = target_link.to(device), target_label.to(device)

            replace_optimizer.zero_grad()
            out = model.meta_prediction(
                source_edge_index, target_edge_index, target_link
            ).squeeze()
            loss_target = criterion(out, target_label).mean()
            loss_target.backward()
            replace_optimizer.step()
        else:
            break
    replace_scheduler.step()


@torch.no_grad()
def evaluate(name, model, source_edge_index, target_edge_index, link, label):
    model.eval()

    out = model(source_edge_index, target_edge_index, link, is_source=False).squeeze()
    try:
        auc = roc_auc_score(label.tolist(), out.tolist())
    except:
        auc = 1.0
    logging.info(f"{name} AUC: {auc:4f}")

    model.train()
    return auc
def get_test_positive_dict(data):
    """
    æ ¹æ“š test linkï¼ˆdata.target_test_linkï¼‰å»ºç«‹ test set user çš„æ­£æ¨£æœ¬å­—å…¸ã€‚
    å›å‚³: {user_id: [item1, item2, ...]}
    """
    test_user_item_dict = {}
    test_link = data.target_test_link.cpu()
    for u, i in zip(test_link[0], test_link[1]):
        u, i = u.item(), i.item()
        if u not in test_user_item_dict:
            test_user_item_dict[u] = []
        test_user_item_dict[u].append(i)
    return test_user_item_dict

def evaluate_hit_ratio(
    model, data, source_edge_index, target_edge_index,
    top_k, num_candidates=99,
    device=None
):
    import random
    model.eval()
    hit_count = 0
    #all_target_items = set(range(data.num_target_items))
    # âœ… æ”¹æˆé€™è¡Œ  
    all_target_items = set(range(data.num_users, data.num_users + data.num_target_items))

    # âœ… å–å¾— test set çš„ user -> positive items å°æ‡‰é—œä¿‚
    user_interactions = get_test_positive_dict(data)
    sim_users = list(user_interactions.keys())  # ç›´æ¥ä½¿ç”¨ test set çš„ user
    print(f"âœ… Test set user count: {len(sim_users)}")

    total_users = 0
    source_edge_index = source_edge_index.to(device)
    target_edge_index = target_edge_index.to(device)

    with torch.no_grad():
        for user_id in sim_users:
            pos_items = user_interactions.get(user_id, set())
            if len(pos_items) > 1:
                print(f"âš ï¸ Warning: User {user_id} has {len(pos_items)} positives in test set.")

            if len(pos_items) == 0:
                continue

            # âœ… ç¬¬ä¸€æ­¥ï¼šé¸æ“‡ä¸€å€‹æ­£æ¨£æœ¬
            pos_item = list(pos_items)[0]
            # print(f"\n=== [User {user_id}] ===")
            # print(f"ğŸ‘‰ Positive item: {pos_item}")

            # âœ… ç¬¬äºŒæ­¥ï¼šæŒ‘é¸è² æ¨£æœ¬ï¼ˆå¾éæ­£æ¨£æœ¬ä¸­éš¨æ©ŸæŠ½ num_candidates å€‹ï¼‰
            negative_pool = list(all_target_items - set(pos_items))
            if len(negative_pool) < num_candidates:
                # print(f"âŒ Negative pool too small for user {user_id}, skipping.")
                continue

            sampled_negatives = random.sample(negative_pool, num_candidates)
            # print(f"ğŸ¯ Sampled {num_candidates} negatives: {sampled_negatives[:10]}...")

            # âœ… ç¬¬ä¸‰æ­¥ï¼šçµ„æˆå€™é¸æ¸…å–®ï¼ˆæ­£ä¾‹ + è² ä¾‹ï¼‰ï¼Œä¸¦æ‰“äº‚
            candidate_items = sampled_negatives + [pos_item]
            random.shuffle(candidate_items)
            # print(f"ğŸ§® Candidate items (shuffled): {candidate_items[:10]}...")

            # âœ… ç¬¬å››æ­¥ï¼šè½‰æˆ tensor ä¸¦é€å…¥æ¨¡å‹è¨ˆç®—åˆ†æ•¸
            user_tensor = torch.tensor([user_id] * len(candidate_items), device=device)
            item_tensor = torch.tensor(candidate_items, device=device)
            link = torch.stack([user_tensor, item_tensor], dim=0)

            scores = model(source_edge_index, target_edge_index, link, is_source=False).squeeze()
            top_k_indices = torch.topk(scores, k=top_k).indices.tolist()
            top_k_items = [candidate_items[i] for i in top_k_indices]

            # print(f"ğŸ“ˆ Top-{top_k} prediction: {top_k_items}")
            # print(f"âœ”ï¸ Hit? {'Yes âœ…' if pos_item in top_k_items else 'No âŒ'}")

            if pos_item in top_k_items:
                hit_count += 1
            total_users += 1

    hit_ratio = hit_count / total_users if total_users > 0 else 0.0
    logging.info(f"[HIT_RATIO@{top_k}] Users={total_users}, Hits={hit_count}, Hit Ratio={hit_ratio:.4f}")
    return hit_ratio

# ğŸ” çµ±è¨ˆæ¯å€‹ cold item åœ¨ test set ä¸­å‡ºç¾çš„æ¬¡æ•¸ï¼ˆæœ‰å¹¾å€‹ user è²·éï¼‰
def count_cold_item_occurrences(data, cold_item_set):
    item_count = {item: 0 for item in cold_item_set}
    test_link = data.target_test_link.cpu().numpy()
    for u, i in zip(*test_link):
        if i in cold_item_set:
            item_count[i] += 1
    return item_count

def find_cold_item_strict(data, target_train_edge_index, target_test_edge_index):
    import numpy as np
    from collections import Counter

    train_edges = target_train_edge_index.cpu().numpy()
    test_edges = target_test_edge_index.cpu().numpy()
    overlap_users = set(data.raw_overlap_users.cpu().numpy())  # â¬…ï¸ overlap user list

    # Step 1: çµ±è¨ˆ overlap user åœ¨ test set ä¸­é»æ“Šçš„ item æ¬¡æ•¸
    test_user, test_item = test_edges
    item_counter = Counter()

    for u, i in zip(test_user, test_item):
        if u in overlap_users:
            item_counter[i] += 1

    candidate_items = {i for i, cnt in item_counter.items() if cnt == 1}

    train_items = set(train_edges[1])
    test_items = set(test_item)

    cold_items = [i for i in candidate_items if i not in train_items and i in test_items]

    if not cold_items:
        print("âŒ æ‰¾ä¸åˆ°ç¬¦åˆæ¢ä»¶çš„ cold item")
        return None

    selected = cold_items[0]
    print(f"ğŸ§Š Found cold item: {selected}")
    return selected

def evaluate_er_hit_ratio(
    model, data, source_edge_index, target_edge_index,
    cold_item_set,
    top_k, num_candidates=99,
    device=None
):
    import random
    model.eval()

    #all_target_items = set(range(data.num_target_items))
    # âœ… æ”¹æˆé€™è¡Œ  
    all_target_items = set(range(data.num_users, data.num_users + data.num_target_items))
    user_interactions = get_test_positive_dict(data)
    sim_users = list(user_interactions.keys())

    source_edge_index = source_edge_index.to(device)
    target_edge_index = target_edge_index.to(device)

    total_users = 0
    cold_item_hit_count = 0
    cold_item_ranks = []  # â¬…ï¸ å„²å­˜ cold item è¢«æ’é€²å»æ™‚çš„æ’å

    with torch.no_grad():
        for user_id in sim_users:
            # å»ºç«‹å€™é¸æ± 
            negative_pool = list(all_target_items - cold_item_set)
            if len(negative_pool) < num_candidates:
                continue

            sampled_items = random.sample(negative_pool, num_candidates)
            sampled_items += list(cold_item_set)
            sampled_items = list(set(sampled_items))
            random.shuffle(sampled_items)

            user_tensor = torch.tensor([user_id] * len(sampled_items), device=device)
            item_tensor = torch.tensor(sampled_items, device=device)
            link = torch.stack([user_tensor, item_tensor], dim=0)

            scores = model(source_edge_index, target_edge_index, link, is_source=False).squeeze()
            scores_list = scores.tolist()

            # å°å‡ºæ¯å€‹ item çš„åˆ†æ•¸
            # print(f"\n=== [User {user_id}] ===")
            # for item, score in zip(sampled_items, scores_list):
            #     tag = "ğŸ§Š COLD" if item in cold_item_set else ""
            #     print(f"Item {item:4d} | Score: {score:.4f} {tag}")

            # è¨ˆç®—æ’åº
            item_score_pairs = list(zip(sampled_items, scores_list))
            item_score_pairs.sort(key=lambda x: x[1], reverse=True)
            sorted_items = [item for item, _ in item_score_pairs]

            # å°å‡º cold item çš„æ’å
            for cold_item in cold_item_set:
                if cold_item in sorted_items:
                    rank = sorted_items.index(cold_item) + 1
                    # print(f"ğŸ” Cold item {cold_item} ranked #{rank} / {len(sorted_items)}")

            top_k_items = sorted_items[:top_k]


            # â¬‡ï¸ çµ±è¨ˆå‘½ä¸­èˆ‡æ’å
            cold_hits = [item for item in top_k_items if item in cold_item_set]
            if cold_hits:
                cold_item_hit_count += 1
                for cold_item in cold_hits:
                    rank = top_k_items.index(cold_item) + 1  # 1-based rank
                    cold_item_ranks.append(rank)

            total_users += 1

    er_ratio = cold_item_hit_count / total_users if total_users > 0 else 0.0
    avg_rank = sum(cold_item_ranks) / len(cold_item_ranks) if cold_item_ranks else -1
    median_rank = (
        sorted(cold_item_ranks)[len(cold_item_ranks) // 2] if cold_item_ranks else -1
    )

    logging.info(f"[ER@{top_k}] Users={total_users}, Cold Item Hits={cold_item_hit_count}, ER Ratio={er_ratio:.4f}")
    # logging.info(f"[ER@{top_k}] Cold item avg rank: {avg_rank:.2f}, median rank: {median_rank}")

    return er_ratio


def evaluate_multiple_topk(model, data, source_edge_index, target_edge_index, cold_item_set, device):
    topk_list = [10, 15, 20, 25, 30, 40, 50, 60, 70, 80, 90, 100]
    print("\nğŸ“Š Evaluation for multiple top-K values:")
    for k in topk_list:
        hr = evaluate_hit_ratio(
            model=model,
            data=data,
            source_edge_index=source_edge_index,
            target_edge_index=target_edge_index,
            top_k=k,
            num_candidates=99,
            device=device
        )

        er = evaluate_er_hit_ratio(
            model=model,
            data=data,
            source_edge_index=source_edge_index,
            target_edge_index=target_edge_index,
            cold_item_set=cold_item_set,
            top_k=k,
            num_candidates=99,
            device=device
        )


def _get_user_embedding_table(model):
    # ä¾åºå˜—è©¦å¸¸è¦‹å‘½åï¼Œæ‰¾åˆ°å°±å›å‚³å°æ‡‰ nn.Embedding
    for name in ["user_embedding", "user_emb", "users_embedding"]:
        if hasattr(model, name):
            return getattr(model, name)
    raise AttributeError("No user embedding table found on model. "
                         "Tried: user_embedding, user_emb, users_embedding")


def cross_domain_align_loss(model, num_users, align_weight=1e-2, detach_source=True, device=None):
    idx = torch.arange(num_users, device=device)
    E_now = model.user_embedding(idx)
    if hasattr(model, "_sgl_user_anchor"):
        E_src = model._sgl_user_anchor[idx]
        if detach_source:
            E_src = E_src.detach()
        return align_weight * F.mse_loss(E_now, E_src)
    return torch.tensor(0.0, device=device)

def find_hard_users(source_user_embs, group_A_emb_idx, top_ratio=0.1):
    """
    æ ¹æ“š source domain ç”¨æˆ¶ embeddingï¼Œæ‰¾å‡º group B ä¸­èˆ‡ group A è·é›¢æœ€å¤§ top_ratio çš„ hard usersã€‚
    Args:
        source_user_embs: Tensor, shape [num_users, emb_dim]
        group_A_emb_idx: LongTensorï¼Œgroup A ç”¨æˆ¶ embedding index
        top_ratio: floatï¼Œå–æœ€å¤§è·é›¢ top å¤šå°‘æ¯”ä¾‹
    Returns:
        hard_user_emb_idx: LongTensorï¼Œgroup B ä¸­è¢«æŒ‘é¸ç‚º hard user çš„ embedding index
    """
    num_users = source_user_embs.size(0)
    device = source_user_embs.device
    all_user_ids = torch.arange(num_users, device=device) #æ§‹å»ºæ‰€æœ‰ç”¨æˆ¶çš„æ¨™æº–ç´¢å¼•å¼µé‡ [0, 1, ..., num_users-1]

    # Group B æ˜¯æ‰€æœ‰ç”¨æˆ¶æ‰£æ‰ Group A
    group_B_mask = ~torch.isin(all_user_ids, group_A_emb_idx.to(device))
    group_B_user_ids = all_user_ids[group_B_mask]  # embedding idx
    

    # L2 æ­£è¦åŒ– embeddingï¼Œé¿å…è·é›¢è¨ˆç®—éç¨‹ä¸­å› å¤§å°ä¸åŒå°è‡´èª¤å·®ï¼Œé©åˆè¨ˆç®— cosine ç›¸ä¼¼åº¦
    group_A_embs = F.normalize(source_user_embs[group_A_emb_idx], p=2, dim=1)
    group_B_embs = F.normalize(source_user_embs[group_B_user_ids], p=2, dim=1)

    # è¨ˆç®— cosine ç›¸ä¼¼åº¦çŸ©é™£
    cosine_sim = torch.matmul(group_B_embs, group_A_embs.T) #shape: [len(group_B), len(group_A)]
    # è½‰æ›ç‚ºè·é›¢ï¼šè·é›¢è¨­ç‚º 1 - cosine similarity
    cosine_dist = 1 - cosine_sim  # è·é›¢è¶Šå¤§è¡¨ç¤ºç”¨æˆ¶è¡Œç‚ºè¶Šä¸ç›¸ä¼¼

    # å°å‡ºéƒ¨åˆ† cosine similarity ç´°ç¯€ä¾›æª¢è¦–
    logging.info(f"Cosine similarity matrix shape: {cosine_sim.shape}")
    logging.info(f"Cosine similarity sample (first 5 Group B users vs first 5 Group A users):\n{cosine_sim[:5, :5]}")

    # æ¯å€‹ Group B ç”¨æˆ¶å–è·é›¢æœ€å°å€¼ (èˆ‡æœ€è¿‘çš„ Group A ç”¨æˆ¶è·é›¢)
    min_dist_per_B_user, min_idx = torch.min(cosine_dist, dim=1)
    max_sim_per_B_user = cosine_sim[torch.arange(len(min_idx), device=device), min_idx] #é€™æ²’ç”¨
    logging.info(f"Sample minimal distances: {min_dist_per_B_user.tolist()}")
    logging.info(f"Sample maximal cosine similarities (closest user): {max_sim_per_B_user[:10].tolist()}") #é€™æ²’ç”¨

    # é–¾å€¼å–æœ€å¤§ top_ratio çš„è·é›¢
    threshold = torch.quantile(min_dist_per_B_user, 1 - top_ratio)
    # ç¯©é¸è·é›¢å¤§æ–¼ç­‰æ–¼é–¾å€¼çš„ Group B ç”¨æˆ¶ä½œç‚º hard users
    hard_mask = min_dist_per_B_user >= threshold
    hard_user_emb_idx = group_B_user_ids[hard_mask]

    logging.info(f"Distance threshold for top {top_ratio*100}% hard users: {threshold:.4f}")
    logging.info(f"Total group B users: {len(group_B_user_ids)}, hard users count: {len(hard_user_emb_idx)}")
    #logging.info(f"Sample hard user embedding idx: {hard_user_emb_idx.tolist()}")

    return hard_user_emb_idx #è¢«é¸ç‚º hard user çš„ Group B ç”¨æˆ¶ embedding ç´¢å¼•

def get_top_items_by_group_A(source_edge_index, group_A_user_ids, top_ratio=0.1):
    user_ids = source_edge_index[0]
    item_ids = source_edge_index[1]

    # æ”¯æ´ list æˆ– Tensor è¼¸å…¥
    if isinstance(group_A_user_ids, torch.Tensor):
        group_A_set = set(group_A_user_ids.cpu().tolist())
    else:
        group_A_set = set(group_A_user_ids)

    # åªä¿ç•™ user å±¬æ–¼ group A çš„é‚Š
    mask = torch.tensor([int(u.item()) in group_A_set for u in user_ids],
                        device=user_ids.device, dtype=torch.bool)
    filtered_items = item_ids[mask].cpu().tolist()

    # ç”¨ collections.Counterï¼Œé¿å…åç¨±é™°å½±
    item_counter = collections.Counter(filtered_items)
    sorted_items = [it for it, _ in item_counter.most_common()]

    top_n = max(1, int(len(sorted_items) * top_ratio))
    return sorted_items[:top_n]


def add_edges_for_hard_users(source_edge_index, hard_user_ids, top_items, device):
    new_user_ids = []
    new_item_ids = []
    for u in hard_user_ids:
        u_id = u.item() if isinstance(u, torch.Tensor) else u
        new_user_ids.extend([u_id] * len(top_items))
        new_item_ids.extend(top_items)
    new_user_tensor = torch.tensor(new_user_ids, dtype=torch.long, device=device)
    new_item_tensor = torch.tensor(new_item_ids, dtype=torch.long, device=device)
    new_edges = torch.stack([new_user_tensor, new_item_tensor], dim=0)
    return torch.cat([source_edge_index, new_edges], dim=1)

def train(model, perceptor, data, args):
    device = args.device
    data = data.to(device)
    model = model.to(device)
    perceptor = perceptor.to(device)

    (
        source_edge_index,
        source_label,
        source_link,
        target_train_edge_index,
        target_train_label,
        target_train_link,
        target_valid_link,
        target_valid_label,
        target_test_link,
        target_test_label,
        target_test_edge_index,
    ) = link_split(data)

    # === SGL åˆå§‹åŒ–ï¼ˆå·²å­˜åœ¨ï¼Œä½†ç¢ºä¿åƒæ•¸æ­£ç¢ºï¼‰===
    if getattr(args, "use_sgl_init", True):
        E_s_u, E_s_i = load_sgl_final_embeddings(args.sgl_dir)
        model = init_bignas_source_from_sgl(
            model=model,
            data=data,
            E_s_u_np=E_s_u,
            E_s_i_np=E_s_i,
            device=device,
            freeze_steps=getattr(args, "freeze_src_steps", 1000),
            # è‹¥æœªåš raw id å°é½Šï¼Œä¸‹é¢å…©å€‹å…ˆä¸å‚³ï¼Œæˆ–å‚³ None
            # user_index_map=...,
            # source_item_index_map=...,
        )
    data.target_test_link = target_test_link

    source_set_size = source_link.shape[1]
    train_set_size = target_train_link.shape[1]
    val_set_size = target_valid_link.shape[1]
    test_set_size = target_test_link.shape[1]
    logging.info(f"Train set size: {train_set_size}")
    logging.info(f"Valid set size: {val_set_size}")
    logging.info(f"Test set size: {test_set_size}")

    #######################################

    # 1. å…ˆå¾ target domain é‚Šå–å¾—æ‰€æœ‰åŸå§‹ç”¨æˆ¶ IDï¼ˆä¸é‡è¤‡ï¼‰
    all_raw_user_ids = target_train_edge_index[0].unique().cpu()

    # 2. å–å¾— raw_overlap_usersï¼šæ¨¡å‹embeddingå°æ‡‰çš„ç”¨æˆ¶åŸå§‹IDé›†åˆï¼ˆå¯ç”¨æ–¼embeddingç´¢å¼•ï¼‰
    overlap_users = data.raw_overlap_users.cpu()  # Tensorï¼Œé•·åº¦2809ï¼Œä¾‹å¦‚
    overlap_users_set = set(overlap_users.numpy())

    # 3. å»ºç«‹æ˜ å°„è¡¨ï¼šraw user id -> embedding ç´¢å¼•ï¼ˆ0~2808ï¼‰
    user_id_to_emb_idx = {uid.item(): idx for idx, uid in enumerate(overlap_users)}

    # 4. æ‰¾ Group A åŸå§‹ç”¨æˆ¶ï¼šè²·éç›®æ¨™å•†å“çš„ç”¨æˆ¶ raw ID
    target_item_id = 3080
    #mask_A = target_train_edge_index[1] == target_item_id
    group_A_raw_user_ids = [50, 98, 118, 191, 260, 550, 735, 947, 1175, 1615]#target_train_edge_index[0][mask_A].cpu()
    print(f"Group A raw user IDs count: {len(group_A_raw_user_ids)}")
    print(f"Group A raw user IDs sample: {group_A_raw_user_ids}") 


    # 5. ç¯©é¸ Group Aï¼Œåªä¿ç•™åœ¨ overlap_users å…§çš„æœ‰æ•ˆ raw user id
    group_A_valid_mask = torch.tensor([uid in overlap_users_set for uid in group_A_raw_user_ids])
    group_A_raw_user_ids_valid = [uid for uid, valid in zip(group_A_raw_user_ids, group_A_valid_mask) if valid]

    # 6. æ˜ å°„ Group A åŸå§‹ç”¨æˆ¶ ID åˆ° embedding ç´¢å¼•
    group_A_user_emb_idx = torch.tensor([user_id_to_emb_idx[uid] for uid in group_A_raw_user_ids_valid])

    # 7. å°æ‰€æœ‰ raw ç”¨æˆ¶ï¼Œç¯©é¸åªåŒ…å« overlap_users çš„æœ‰æ•ˆç”¨æˆ¶ï¼Œå†æ˜ å°„æˆ embedding ç´¢å¼•
    valid_all_mask = torch.tensor([uid in overlap_users_set for uid in all_raw_user_ids])
    valid_all_raw_user_ids = all_raw_user_ids[valid_all_mask]
    all_user_emb_idx = torch.tensor([user_id_to_emb_idx[uid] for uid in valid_all_raw_user_ids])

    # 8. Group B å³æ‰€æœ‰æœ‰æ•ˆç”¨æˆ¶å‰”é™¤ Group Aï¼Œç”¨ embedding ç´¢å¼•å½¢å¼è¡¨ç¤º
    group_B_mask = ~torch.isin(all_user_emb_idx, group_A_user_emb_idx)
    group_B_user_emb_idx = all_user_emb_idx[group_B_mask]

    # åˆ—å°è³‡è¨Šç¢ºèª
    logging.info(f"Group A user count: {len(group_A_user_emb_idx)}")
    logging.info(f"Group B user count: {len(group_B_user_emb_idx)}")
    # logging.info(f"Group A sample embedding idx: {group_A_user_emb_idx[:10].tolist()}")
    # logging.info(f"Group B sample embedding idx: {group_B_user_emb_idx[:10].tolist()}")

    # 9. ä½ å¯ä»¥ç”¨ group_A_user_emb_idx å’Œ group_B_user_emb_idx é€™å…©çµ„embeddingç´¢å¼•å»ç´¢å¼• model.user_embedding.weight åšå¾ŒçºŒè¨ˆç®—
    source_user_embs = model.user_embedding.weight  # shape [num_users, emb_dim]

    # è¨ˆç®—è·é›¢èˆ‡æ‰¾ hard user ç­‰å¾ŒçºŒæ­¥é©Ÿ...
    # æ‰¾å‡ºhard user
    hard_user_ids = find_hard_users(source_user_embs, group_A_user_emb_idx.to(source_user_embs.device), top_ratio=0.1)
    
    #emb_userID to org_userID
    emb_idx_to_user_id = {v: k for k, v in user_id_to_emb_idx.items()}
    hard_user_raw_ids = [emb_idx_to_user_id[idx.item()] for idx in hard_user_ids.cpu()]
    hard_user_raw_ids.sort()
    print(f"ç”¨æˆ¶åŸå§‹IDï¼ˆæ’åºå¾Œï¼‰ï¼š{hard_user_raw_ids}")
    #print(f"æ‰¾åˆ° hard user æ•¸é‡: {len(hard_user_ids)}")
    #print(f"ç¤ºä¾‹ hard user id: {hard_user_ids.tolist()}")#é€™æ˜¯emb_userID
    
    ###################################
    
    # ç¢ºä¿ device ä¸€è‡´
    device = target_train_edge_index.device

    # 1. å°‡ hard user åŸå§‹IDè½‰ tensor
    hard_user_raw_ids_tensor = torch.tensor(hard_user_raw_ids, dtype=torch.long, device=device)

    # # # 2. å»ºç«‹ç›®æ¨™item id tensor (å…¨å¡« target_item_id)
    target_item_ids_tensor = torch.full_like(hard_user_raw_ids_tensor, fill_value=target_item_id)

    # # # 3. å°‡ç”¨æˆ¶å’Œå•†å“åˆä½µæˆé‚Šç´¢å¼•çŸ©é™£
    new_edges = torch.stack([hard_user_raw_ids_tensor, target_item_ids_tensor], dim=0)

    # # # 4. åˆä½µæ–°é‚Šåˆ° target train edge index
    target_train_edge_index = torch.cat([target_train_edge_index, new_edges], dim=1)

    # # # 5. åŒæ­¥åˆä½µæ¨™ç±¤ (å…¨éƒ¨1 = æ­£ä¾‹)
    new_labels = torch.ones(hard_user_raw_ids_tensor.size(0), dtype=target_train_label.dtype, device=device)
    target_train_label = torch.cat([target_train_label, new_labels], dim=0)
    
    # â˜… åŒæ­¥æ›´æ–° linkï¼ˆDataLoader ç”¨çš„æ˜¯ link ä¸æ˜¯ edge_indexï¼‰
    target_train_link = torch.cat([target_train_link, new_edges], dim=1)

    
    # åœ¨é€™è£¡åŠ ä¸Šä½ è¦å°çš„ç¢ºèªè³‡è¨Š
    before_edges = target_train_edge_index.shape[1]
    target_train_edge_index = torch.cat([target_train_edge_index, new_edges], dim=1)
    print(f"åŸå§‹train edge count: {before_edges}")
    print(f"æ–°å¢å¾Œtrain edge count: {target_train_edge_index.shape[1]}")

    
    ##################################
    # åœ¨ source domain æ–°å¢ hard user èˆ‡ç†±é–€å•†å“çš„é‚Š
    ##################################
        
    # === è®€å–è¶…åƒæ•¸ top_ratioï¼Œèª¿æ•´å¾ group A è³¼è²·å•†å“ä¸­é¸å–å¤šå°‘æ¯”ä¾‹ç†±é–€å•†å“åŠ å…¥
    top_ratio = args.source_item_top_ratio

    # === çµ±è¨ˆ group A åœ¨ source domain è³¼è²·çš„ç†±é–€å•†å“
    top_items = get_top_items_by_group_A(source_edge_index, group_A_raw_user_ids_valid, top_ratio=top_ratio)
    logging.info(f"Top {top_ratio*100:.1f}% items in source domain by Group A: {top_items[:10]} (total {len(top_items)})")

    # === æ–°å¢é€™äº›å•†å“é‚Šçµ¦ hard user
    source_edge_index = add_edges_for_hard_users(source_edge_index, hard_user_raw_ids, top_items, device)
    logging.info(f"Source domain edge count before adding: {source_edge_index.shape[1] - len(hard_user_raw_ids)*len(top_items)}")
    logging.info(f"Source domain edge count after adding: {source_edge_index.shape[1]}")
    
    ### æ–°å¢source domainçš„é‚ŠçµæŸ ###
    
    ###################################

    target_train_set = Dataset(
        target_train_link.to("cpu"),
        target_train_label.to("cpu"),
    )
    target_train_loader = DataLoader(
        target_train_set,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        collate_fn=target_train_set.collate_fn,
    )

    source_batch_size = int(args.batch_size * train_set_size / source_set_size)
    source_train_set = Dataset(source_link.to("cpu"), source_label.to("cpu"))
    source_train_loader = DataLoader(
        source_train_set,
        batch_size=source_batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        collate_fn=source_train_set.collate_fn,
    )

    target_meta_loader = DataLoader(
        target_train_set,
        batch_size=args.meta_batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        collate_fn=target_train_set.collate_fn,
    )
    target_meta_iter = iter(target_meta_loader)
    source_meta_batch_size = int(
        args.meta_batch_size * train_set_size / source_set_size
    )
    source_meta_loader = DataLoader(
        source_train_set,
        batch_size=source_meta_batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        collate_fn=source_train_set.collate_fn,
    )

    optimizer = torch.optim.AdamW(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )
    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=args.epochs
    )

    perceptor_optimizer = torch.optim.Adam(
        perceptor.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )
    meta_optimizer = MetaOptimizer(
        meta_optimizer=perceptor_optimizer,
        hpo_lr=args.hpo_lr,
        truncate_iter=3,
        max_grad_norm=10,
    )

    # model_param = [
    #     param for name, param in model.named_parameters() if "preds" not in name
    # ]
    replace_param = [
        param for name, param in model.named_parameters() if name.startswith("replace")
    ]
    replace_optimizer = torch.optim.Adam(replace_param, lr=args.lr)
    replace_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        replace_optimizer, T_max=args.T_max
    )

    early_stopping = EarlyStopping(
        patience=args.patience,
        verbose=True,
        path=args.model_path,
        trace_func=logging.info,
    )

    criterion = nn.BCELoss(reduction="none")
    iteration = 0
    for epoch in range(args.epochs):
        for (source_link, source_label), (target_link, target_label) in zip(
            source_train_loader, target_train_loader
        ):
            # â˜… é€æ­¥è§£é™¤å‡çµï¼ˆè‹¥ä½ åœ¨ init æ™‚è¨­äº† freeze_steps>0ï¼‰
            step_unfreeze_if_ready(model)

            torch.cuda.empty_cache()
            source_link = source_link.to(device)
            source_label = source_label.to(device)
            target_link = target_link.to(device)
            target_label = target_label.to(device)
            weight_source = perceptor(source_link[1], source_edge_index, model)

            optimizer.zero_grad()
            source_out = model(
                source_edge_index, target_train_edge_index, source_link, is_source=True
            ).squeeze()
            target_out = model(
                source_edge_index, target_train_edge_index, target_link, is_source=False
            ).squeeze()
            source_loss = (
                criterion(source_out, source_label).reshape(-1, 1) * weight_source
            ).sum()
            target_loss = criterion(target_out, target_label).mean()

            # â˜… å¯é¸ï¼šä½¿ç”¨è€…å°é½Šæå¤±
            if getattr(args, "use_align", False):
                loss_align = cross_domain_align_loss(
                    model, num_users=data.num_users,
                    align_weight=getattr(args, "align_weight", 1e-2),
                    detach_source=getattr(args, "detach_source", True),
                    device=device
                )
            else:
                loss_align = 0.0

            loss = (source_loss + target_loss if args.use_meta else target_loss) + loss_align

            loss.backward()
            optimizer.step()

            iteration += 1
            if (
                args.use_source
                and args.use_meta
                and iteration % args.meta_interval == 0
            ):
                logging.info(f"Entering meta optimization, iteration: {iteration}")
                meta_optimizeation(
                    target_meta_loader,
                    replace_optimizer,
                    model,
                    args,
                    criterion,
                    replace_scheduler,
                    source_edge_index,
                    target_train_edge_index,
                )

                try:
                    target_meta_link, target_meta_label = next(target_meta_iter)
                except StopIteration:
                    target_meta_iter = iter(target_meta_loader)
                    target_meta_link, target_meta_label = next(target_meta_iter)

                target_meta_link, target_meta_label = (
                    target_meta_link.to(device),
                    target_meta_label.to(device),
                )
                optimizer.zero_grad()
                target_out = model(
                    source_edge_index,
                    target_train_edge_index,
                    target_meta_link,
                    is_source=False,
                ).squeeze()
                meta_loss = criterion(target_out, target_meta_label).mean()

                for (source_link, source_label), (target_link, target_label) in zip(
                    source_meta_loader, target_meta_loader
                ):
                    source_link, source_label = source_link.to(device), source_label.to(
                        device
                    )
                    target_link, target_label = target_link.to(device), target_label.to(
                        device
                    )
                    weight_source = perceptor(source_link[1], source_edge_index, model)

                    optimizer.zero_grad()
                    source_out = model(
                        source_edge_index,
                        target_train_edge_index,
                        source_link,
                        is_source=True,
                    ).squeeze()
                    target_out = model(
                        source_edge_index,
                        target_train_edge_index,
                        target_link,
                        is_source=False,
                    ).squeeze()
                    source_loss = (
                        criterion(source_out, source_label).reshape(-1, 1)
                        * weight_source
                    ).sum()
                    target_loss = criterion(target_out, target_label).mean()
                    meta_train_loss = (
                        source_loss + target_loss if args.use_meta else target_loss
                    )
                    break

                torch.cuda.empty_cache()
                # å–ç›®å‰å¯è¨“ç·´ã€ä¸”ä¸æ˜¯ preds çš„åƒæ•¸
                trainable_params = [
                    p for n, p in model.named_parameters()
                    if ("preds" not in n) and p.requires_grad
                ]

                meta_optimizer.step(
                    train_loss=meta_train_loss,
                    val_loss=meta_loss,
                    aux_params=list(perceptor.parameters()),  # perceptor åƒæ•¸ä¿æŒä¸å‹•
                    parameters=trainable_params,              # é€™è£¡æ”¹æˆ trainable_params
                    return_grads=True,
                    entropy=None,
                )
        train_auc = evaluate(
            "Train",
            model,
            source_edge_index,
            target_train_edge_index,
            target_train_link,
            target_train_label,
        )
        val_auc = evaluate(
            "Valid",
            model,
            source_edge_index,
            target_train_edge_index,
            target_valid_link,
            target_valid_label,
        )

        logging.info(
            f"[Epoch: {epoch}]Train Loss: {loss:.4f}, Train AUC: {train_auc:.4f}, Valid AUC: {val_auc:.4f}"
        )
        wandb.log(
            {
                "loss": loss,
                "train_auc": train_auc,
                "val_auc": val_auc
            },
            step=epoch,
        )

        early_stopping(val_auc, model)
        if early_stopping.early_stop:
            logging.info("Early stopping")
            break

        lr_scheduler.step()

    model = load_model(args).to(device)
    evaluate_hit_ratio(
        model=model,
        data=data,
        source_edge_index=source_edge_index,
        target_edge_index=target_train_edge_index,  # âœ… æ­£ç¢ºå‚³å…¥æ¸¬è©¦é›† edge_index
        top_k=args.top_k,
        num_candidates=99,
        device=device,
    )
    # cold_item_id = find_cold_item_strict(data, target_train_edge_index, target_test_edge_index)
    cold_item_id =17069
    if cold_item_id is not None:
        evaluate_er_hit_ratio(
            model=model,
            data=data,
            source_edge_index=source_edge_index,
            target_edge_index=target_train_edge_index,
            cold_item_set={cold_item_id},
            top_k=args.top_k,
            num_candidates=99,
            device=device,
        )

    # logging.info(f"Hit Ratio (no injection): {pre_hit_ratio:.4f}")
    test_auc = evaluate(
        "Test",
        model,
        source_edge_index,
        target_train_edge_index,
        target_test_link,
        target_test_label,
    )
    logging.info(f"Test AUC: {test_auc:.4f}")
    wandb.log({"Test AUC": test_auc})
    evaluate_multiple_topk(
        model=model,
        data=data,
        source_edge_index=source_edge_index,
        target_edge_index=target_train_edge_index,
        cold_item_set={cold_item_id},   # æ³¨æ„é€™é‚Šæ˜¯ setï¼Œä¸æ˜¯ cold_item_id=
        device=device
    )
        # === å­˜ä¸‹ source_item_embedding ===
    source_emb = model.source_item_embedding.weight.detach().cpu().numpy()
    np.save("source_item_embedding.npy", source_emb)
    np.savetxt("source_item_embedding.csv", source_emb, delimiter=",")
    logging.info(f"âœ… Saved source_item_embedding: shape={source_emb.shape}")